{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling the VAE\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "I wrote a little helper class to make it easier to sample strings from the variational autoencoder (VAE) model—in particular, models trained with tokens and embeddings from [BPEmb](https://nlp.h-its.org/bpemb/). This notebook takes you through the functionality, using the `poetry_1m_sample` model I trained (see README for download instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, importlib\n",
    "import torch\n",
    "from vaesampler import BPEmbVaeSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the configuration and assign the parameters to a `Namespace` object. Then, create the `BPEmbVaeSampler` object with the same `bpemb` parameters used to train the model and the path to the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allison/Dropbox/projects/vae-lagging-encoder/env/lib/python3.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "config_file = \"config.config_poetry_1m_sample\"\n",
    "params = argparse.Namespace(**importlib.import_module(config_file).params)\n",
    "bpvs = BPEmbVaeSampler(lang=params.bpemb['lang'], vs=params.bpemb['vs'], dim=params.bpemb['dim'],\n",
    "                       decode_from=\"./models/poetry_1m_sample/2019-08-20T03:32:25.569351-012.pt\",\n",
    "                       params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = params.nz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "The main thing you'll want to do is decode strings from a latent variable `z`. This variable has a Gaussian distribution (or at least it *should*—that's the whole point of a VAE, right?). There are three methods for decoding strings from `z`:\n",
    "\n",
    "* `.sample()` samples the (softmax) distribution of the output with the given temperature at each step;\n",
    "* `.greedy()` always picks the most likely next token;\n",
    "* `.beam()` expands multiple \"branches\" of the output and returns the most likely branch\n",
    "\n",
    "(These methods use the underlying implementations in the `LSTMDecoder` class provided in the original repository.)\n",
    "\n",
    "Below you'll find some examples of each. First, `.sample()` with a temperature of 0.5. (Increase the temperature for more unlikely output; it approximates `.greedy()` as the temperature approaches 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why could see me, poor dog shall you be;\n",
      "Six swivey-leaved thee of doth showers\n",
      "As gentle lips they and the angels still.\n",
      "But, in I've brav'd to,\n",
      "Turn, and the clouds of heaven,\n",
      "And now, shall weep, we cannot know,\n",
      "And smiling, while my soul goes down\n",
      "Swall and from the hills were the barley,\n",
      "O glorious man, as lovely of man's foe.\n",
      "The still of virtue bright.\n",
      "Are not--let a scornful, as the sun.\n",
      "In a feast that they were over the sky,\n",
      "And in the voice of them blew;\n",
      "I sighed and still thou art thou see!\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"\\n\".join(bpvs.sample(torch.randn(14, z_size), temperature=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy decoding (usually less interesting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took the heart of mine,\n",
      "And tenderly on a blush of a sighing throng.\n",
      "So learned in this other other,\n",
      "Did weeping to be sorely say:\n",
      "Exulted their solemn,\n",
      "Which the eternal state of man.\n",
      "Then I'm ready to-day, and, \"I want to eat\n",
      "Only a voice so loud,\n",
      "Those primal of their loosened ground.\n",
      "With thirst, as far, than more than more than more.\n",
      "And a distant circle of a little space,\n",
      "The lion of the yonder cavalier.\n",
      "The light of the water-trees of gold.\n",
      "The thousand years of arcady;\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"\\n\".join(bpvs.greedy(torch.randn(14, z_size))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search (a good compromise, but slow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience of whom I pray,\n",
      "Such scorns'd and sighs and fro.\n",
      "And greater things so much\n",
      "I know not, old man, and I've got to see,\n",
      "A poet was not to the same\n",
      "And in the boughs of his bosom lay\n",
      "Who canst not be contented,\n",
      "The roar of a lovely heart.\n",
      "But in this time that indignation\n",
      "He saw a lion, who was a mountain tree,\n",
      "It was a power, a happy truth!\n",
      "That tints of pity of _me_ _plend_\n",
      "Lest it's better thank'd, a year.\n",
      "And aught I could not have got his heart in a sight.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"\\n\".join(bpvs.beam(torch.randn(14, z_size), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homotopies (linear interpolation)\n",
    "\n",
    "Using the VAE, you can explore linear interpolations between two lines of poetry. The code in the cell below picks two points at random in the latent space and decodes at evenly-spaced points between the two. (I've included commented-out calls to different decoding methods do make it easy to experiment with them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot live and sing your fairy-time,\n",
      "I cannot live and sing your fairy-time,\n",
      "We live in all the folks of my ears,\n",
      "We are all that flowers and the fairy-time,\n",
      "We are the flowers of my ears of sorrow,\n",
      "Will all the flowers of their joys and song,\n",
      "Their hearts of the flowers of myriads of song,\n",
      "Their hearts of the flowers of their love and cheer,\n",
      "Their heads of the hopes of that and sorrow,\n",
      "Their mouth of the hopes of that and love and fears\n",
      "Their mouth of the hopes of that and love and fears\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn(1, z_size)\n",
    "    y = torch.randn(1, z_size)\n",
    "    steps = 10\n",
    "    for i in range(steps + 1):\n",
    "        z = (y * (i/steps)) + (x * (1-(i/steps)))\n",
    "        print(bpvs.greedy(z)[0])\n",
    "        #print(bpvs.sample(z, 0.35)[0])\n",
    "        #print(bpvs.beam(z, 3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this same logic, you can produce variations on a line of poetry by adding a bit of random noise to the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under my bonny and the sweet of flowers\n",
      "Under a old old hours of the flowers\n",
      "Spreading a old old song of the flowers\n",
      "After a old old hours of the old\n",
      "After a old old and of the flowers\n",
      "Under a bonny of the morning of\n",
      "Under a lute of the morning of\n",
      "Under my bonny and the sweetest\n",
      "After a old old hours of the fair\n",
      "Under a old old song of the old\n",
      "Under a old old and of a dream\n",
      "Under a old old and of the flowers\n",
      "Under a bonny of the sweet and fair\n",
      "Under the old old song of the flowers\n",
      "Under a bonny of the sweet and fair\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn(1, z_size)\n",
    "    steps = 14\n",
    "    for i in range(steps + 1):\n",
    "        z = x + (torch.randn(1, z_size)*0.1)\n",
    "        print(bpvs.greedy(z)[0])\n",
    "        #print(bpvs.sample(z, 0.35)[0])\n",
    "        #print(bpvs.beam(z, 4)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested by [@halcy@icosahedron.website](https://icosahedron.website/@halcy/102650042038601749): decoding from points on a randomly-selected circular path (halcy notes that this is \"actually 'only' an ellipse unless ab and ac are orthogonal, which for high dimensional vectors picked randomly is pretty likely to be approximately true\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def circ_generator(a, b, c, steps, radius=1):\n",
    "    lerp = np.linspace(0, 1, steps).reshape(-1, 1)\n",
    "    axis_x = (a - b).flatten() / np.linalg.norm(a - b)\n",
    "    axis_y = (a - c).flatten() / np.linalg.norm(a - c)\n",
    "    latents_x = np.sin(math.pi * 2.0 * lerp) * radius\n",
    "    latents_y = np.cos(math.pi * 2.0 * lerp) * radius\n",
    "    latents = a + (latents_x * axis_x) + (latents_y * axis_y)\n",
    "    return torch.tensor(latents).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all my own.\n",
      "And not her own.\n",
      "And held his clothes and her.\n",
      "And held his anxious hands.\n",
      "And his mouth, silently are torn.\n",
      "Smote, a brimming pearls,\n",
      "Quick dazzling, gleaming blaze,\n",
      "Hast thou, like a flowery waters\n",
      "Do thou not, like a happy day\n",
      "We shall not not in this.\n",
      "For you to my own.\n",
      "For all my own.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = np.random.randn(z_size)\n",
    "    b = np.random.randn(z_size)\n",
    "    c = np.random.randn(z_size)\n",
    "    circ = circ_generator(a, b, c, 12, i)\n",
    "    #out = bpvs.greedy(circ)\n",
    "    #out = bpvs.sample(circ, 0.5)\n",
    "    out = bpvs.beam(circ, 4)\n",
    "    print(\"\\n\".join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructions\n",
    "\n",
    "You can ask the model to produce the latent vector for any given input. (Using `BPEmb` helps ensure that arbitrary inputs won't fail because of out-of-vocabulary tokens.) The latent vector is given as a Gaussian distribution—a mean (`mu`) and a variance. You can either sample from this distribution with `.z()` or just get the mean with `.mu()`.\n",
    "\n",
    "You can then pass this to `.sample()`, `.beam()`, or `.greedy()` to produce a string. The model's reconstructions aren't super accurate, but you can usually see some hint of the original string's meaning or structure in the output. Here I'm experimenting with H.D.'s 1916 poem \"Sea Rose\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "strs = \"\"\"\\\n",
    "Rose, harsh rose, \n",
    "marred and with stint of petals, \n",
    "meagre flower, thin, \n",
    "spare of leaf,\n",
    "more precious \n",
    "than a wet rose \n",
    "single on a stem -- \n",
    "you are caught in the drift.\n",
    "Stunted, with small leaf, \n",
    "you are flung on the sand, \n",
    "you are lifted \n",
    "in the crisp sand \n",
    "that drives in the wind.\n",
    "Can the spice-rose \n",
    "drip such acrid fragrance \n",
    "hardened in a leaf?\"\"\".split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the original poem along with its reconstruction, calculating from the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rose, harsh rose,                  Rose, like a sudden, and\n",
      "marred and with stint of petals,   Marred and a hundred miles of gold,\n",
      "meagre flower, thin,               Dripping, sweet, and fair\n",
      "spare of leaf,                     Little a thousand, as a thousand way\n",
      "more precious                      More precious as a pilgrim's heart\n",
      "than a wet rose                    As a little wind was silent in\n",
      "single on a stem --                Without a leaf of a thousand-born\n",
      "you are caught in the drift.       You are not in the western sea.\n",
      "Stunted, with small leaf,          Stunted, like a thousand-hearted,\n",
      "you are flung on the sand,         You are not on the sandals,\n",
      "you are lifted                     They are not a thousand-eyed hand\n",
      "in the crisp sand                  In the crisp of silver-waves\n",
      "that drives in the wind.           That opens in the sky. He saw\n",
      "Can the spice-rose                 Let the grape-tree of a way\n",
      "drip such acrid fragrance          Dripping a golden goblet\n",
      "hardened in a leaf?                Hardened in a leafy tree;\n"
     ]
    }
   ],
   "source": [
    "llen = max([len(item) for item in strs])\n",
    "with torch.no_grad():\n",
    "    sampled = bpvs.greedy(bpvs.mu(strs))\n",
    "    for orig, line in zip(strs, sampled):\n",
    "        print(orig.ljust(llen+1), line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A beam search based on a sample from the latent Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rose, harsh rose,                  Fearless, swift and helpless,\n",
      "marred and with stint of petals,   Radiant in a thousand miles,\n",
      "meagre flower, thin,               To pleasing, so, like some\n",
      "spare of leaf,                     Little odors, like a lovely face\n",
      "more precious                      Most sweetly horrible of the sun,\n",
      "than a wet rose                    As they are slumbered and burning\n",
      "single on a stem --                Without a passage from thee\n",
      "you are caught in the drift.       At length on the floor of barley;\n",
      "Stunted, with small leaf,          Crept, like a tropic sphere\n",
      "you are flung on the sand,         Shall we have flung on the ground,\n",
      "you are lifted                     You are not a garlanded of wine;\n",
      "in the crisp sand                  With the flaunts of flowers\n",
      "that drives in the wind.           That kissed in the dark surprise;\n",
      "Can the spice-rose                 Come to the lark-trees\n",
      "drip such acrid fragrance          A single bird a pallid tree\n",
      "hardened in a leaf?                Her face was a golden birch-tree;\n"
     ]
    }
   ],
   "source": [
    "llen = max([len(item) for item in strs])\n",
    "with torch.no_grad():\n",
    "    sampled = bpvs.beam(bpvs.z(strs), 4)\n",
    "    for orig, line in zip(strs, sampled):\n",
    "        print(orig.ljust(llen+1), line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And rewriting the poem, line by line, sampling the softmax layer with increasing temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little,\n",
      "Patience and the laughter of decay,\n",
      "Tearing a little tone.\n",
      "Suddenly a noise.\n",
      "Immortal hearts\n",
      "Canto.\n",
      "Like a silver sea!\n",
      "Are that are into the world.\n",
      "Olt on, one mounting,\n",
      "Were broken de on the hook below,\n",
      "Although that implicit ...\n",
      "And comparable admission, thames...\n",
      "This carl to my skies;\n",
      "Lucia-building grows spring)\n",
      "Rip ⁇  peuch anthio cam nurinement\n",
      "Atlantis stars a laugh dish investments!\n"
     ]
    }
   ],
   "source": [
    "max_temp = 2.0\n",
    "with torch.no_grad():\n",
    "    for i, line in enumerate(strs):\n",
    "        sampled = bpvs.sample(bpvs.z([line]),\n",
    "                              max_temp * (i/len(strs)) + 1e-5)\n",
    "        print(sampled[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations on a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy's heart is cold asleep\n",
      "And chanced and gazed a thousand years\n",
      "His father's heart is red asleep\n",
      "His fellow-shodding as I am free\n",
      "The hint of death-swepted eyes\n",
      "The shepherd's heart is flashed in air\n",
      "The nymphs in deathless and sweet\n",
      "The maiden's heart-sheaves of mine\n",
      "His woe's heart was cold asleep\n",
      "His sighs waking in a sudden throng\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    center = bpvs.mu([\"My cat's breath smells like cat food\"])\n",
    "    print(bpvs.sample(center, 0.35)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And interpolating between two specified lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "Three years on the casement of her,\n",
      "Three years on his casement and the trees,\n",
      "Seven days on his plate and a green,\n",
      "For every side of the tangled,\n",
      "For one in his hand of wine,\n",
      "For every side in his own meat,\n",
      "For that he lies with a bee,\n",
      "For every side of his own bread,\n",
      "For every side of his own care,\n",
      "And in my own a single food,\n",
      "And that his purpose is not.\n",
      "And in my purpose is not.\n",
      "And that was not a single thing.\n",
      "And that has made all the difference.\n"
     ]
    }
   ],
   "source": [
    "start_s = \"Two roads diverged in a yellow wood,\"\n",
    "end_s = \"And that has made all the difference.\"\n",
    "with torch.no_grad():\n",
    "    x = bpvs.z([start_s])\n",
    "    y = bpvs.z([end_s])\n",
    "    steps = 12\n",
    "    print(start_s)\n",
    "    for i in range(steps + 1):\n",
    "        z = (y * (i/steps)) + (x * (1-(i/steps)))\n",
    "        print(bpvs.sample(z, 0.25)[0])\n",
    "        #print(bpvs.greedy(z)[0])\n",
    "        #print(bpvs.beam(z, 4)[0])\n",
    "    print(end_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
