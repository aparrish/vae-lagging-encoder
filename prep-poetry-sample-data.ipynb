{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a poetry corpus for training\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "I wanted to train a VAE with [BPEmb](https://nlp.h-its.org/bpemb/)'s pretrained sub-word embeddings. This notebook helps create the dataset. First, install `bpemb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bpemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `BPEmb` with the desired vocabulary size. The `BPEmb` object downloads the models and embeddings on demand, so this might take a while the first time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpemb\n",
    "import json, gzip, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bp = bpemb.BPEmb(lang='en', dim=200, vs=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [Project Gutenberg Poetry Corpus](https://github.com/aparrish/gutenberg-poetry-corpus) and change the path below to its location on your drive. The following loads in all ~3M lines of poetry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for line in gzip.open(\"/Users/allison/projects/gutenberg-dammit-archive/gutenberg-poetry-v001.ndjson.gz\"):\n",
    "    data = json.loads(line.strip())\n",
    "    lines.append(data['s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085117"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the name of the dataset and the size of the training, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'poetry_10k_sample'\n",
    "train_size = 10000\n",
    "valid_size = 1000\n",
    "test_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then write out the files, using `bpemb` to encode to the fixed vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets/{dataset_name}_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"datasets/%s_data/%s.train.txt.gz\" % (dataset_name, dataset_name), \"wt\") as fh:\n",
    "    for line in lines[:train_size]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"datasets/%s_data/%s.valid.txt.gz\" % (dataset_name, dataset_name), \"wt\") as fh:\n",
    "    for line in lines[train_size:train_size+valid_size]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"datasets/%s_data/%s.test.txt.gz\" % (dataset_name, dataset_name), \"wt\") as fh:\n",
    "    for line in lines[train_size+valid_size:train_size+valid_size+test_size]:\n",
    "        fh.write(' '.join(bp.encode(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/%s_data/vocab.txt\" % dataset_name, \"w\") as fh:\n",
    "    for item in bp.words:\n",
    "        fh.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train the model as normal with this data. Create a file `config/config_YOUR_DATASET_NAME.py` with your desired hyperparameters and then train with the commands discussed in the README. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
